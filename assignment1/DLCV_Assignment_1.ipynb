{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KE1Rct79pdT1"
   },
   "source": [
    "# DLCV Assignment 1\n",
    "\n",
    "**Due Date: 22/02/2024 11:59PM IST**\n",
    "\n",
    "**Name:** Rishav Saha\n",
    "\n",
    "**Sr. No.:** 22573\n",
    "\n",
    "\n",
    "In this assignment, we will cover the following topics:\n",
    "\n",
    "1) Training a simple Linear Model \n",
    "\n",
    "2) Implementing Modules with Backprop functionality\n",
    "\n",
    "3) Implementing Convolution Module on Numpy\n",
    "\n",
    "\n",
    "It is crucial to get down to the nitty gritty of the code to implement all of these. No external packages (like caffe,pytorch etc), which directly give functions for these steps, are to be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Emb3Bo_NpdUl"
   },
   "source": [
    "# Training a simple Linear Model\n",
    "\n",
    "In this section, you will write the code to train a Linear Model. The goal is to classify an input $X_i$ of size $n$ into one of $m$ classes. For this, you need to consider the following:\n",
    "\n",
    "1)  **Weight Matrix** $W_{n\\times m}$: The Weights are multipled with the input $X_i$ (vector of size $n$), to find $m$ scores $S_m$ for the $m$ classes.\n",
    "\n",
    "2)  **The Loss function**:   \n",
    "  * The Cross Entropy Loss: By interpreting the scores as unnormalized log probabilities for each class, this loss tries to measure dissatisfaction with the scores in terms of the log probability of the right class:\n",
    "\n",
    "$$\n",
    "L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}\n",
    "$$\n",
    "\n",
    "where $f_{ y_i }$ is the $y_i$-th element of the output of $W^T  X_i$\n",
    "\n",
    "3) **A Regularization term**: In addition to the loss, you need a Regularization term to lead to a more distributed (in case of $L_2$) or sparse (in case of $L_1$) learning of the weights. For example, with $L_2$ regularization, the loss has the following additional term:\n",
    "\n",
    "$$\n",
    "R(W) = \\sum_k\\sum_l W_{k,l}^2  \n",
    "$$\n",
    "\n",
    "Thus the total loss has the form:\n",
    "$$\n",
    "L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss} \\\\\\\\\n",
    "$$\n",
    "\n",
    "4) **An Optimization Procedure**: This refers to the process which tweaks the weight Matrix $W_{n\\times m}$ to reduce the loss function $L$. In our case, this refers to Mini-batch Gradient Descent algorithm. We adjust the weights $W_{n\\times m}$, based on the gradient of the loss $L$ w.r.t. $W_{n\\times m}$. This leads to:\n",
    "$$\n",
    "W_{t+1} = W_{t} - \\alpha \\frac{\\partial L}{\\partial W},\n",
    "$$\n",
    "where $\\alpha$ is the learning rate. Additionally, with \"mini-batch\" gradient descent, instead of finding loss over the whole dataset, we use a small sample $B$ of the training data to make each learning step. Hence,\n",
    "$$\n",
    "W_{t+1} = W_{t} - \\alpha \\frac{\\partial \\sum_{i \\in B}{L_{x_i}}}{\\partial W},\n",
    "$$\n",
    "where $|B|$ is the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2neGQlEpdVD"
   },
   "source": [
    "# Question 1\n",
    "\n",
    "Train a **Single-Layer Classifier** for the MNIST dataset. \n",
    "* Use Softmax-Loss.\n",
    "* Maintain a train-validation split of the original training set for finding the right value of $\\lambda$ for the regularization, and to check for over-fitting.\n",
    "* Finally, evaluate the classification performance on the test-set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwa72NJTpdVc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n"
     ]
    }
   ],
   "source": [
    "## Load The Mnist data:\n",
    "# Download data from http://yann.lecun.com/exdb/mnist/\n",
    "# load the data.\n",
    "import idx2numpy\n",
    "data=idx2numpy.convert_from_file('train-images.idx3-ubyte')\n",
    "label=idx2numpy.convert_from_file('train-labels.idx1-ubyte')\n",
    "test_data=idx2numpy.convert_from_file('t10k-images.idx3-ubyte')\n",
    "test_label=idx2numpy.convert_from_file('t10k-labels.idx1-ubyte')\n",
    "# maintain a train-val split\n",
    "train_size=0.7*len(data)\n",
    "training_x=data[0:int(train_size)]\n",
    "print(len(training_x))\n",
    "crossval_x=data[int(train_size):len(data)]\n",
    "training_y=label[0:int(train_size)]\n",
    "crossval_y=label[int(train_size):len(data)]\n",
    "# Now, write a generator that yields (random) mini-batches of the input data\n",
    "# Do not use same set of mini-batches for different epochs\n",
    "    \n",
    "def get_minibatch(training_x=training_x, training_y=training_y):\n",
    "    ## Read about Python generators if required.\n",
    "\n",
    "    ## WRITE CODE HERE\n",
    "    batch_size=100\n",
    "    num_samples = len(training_x)\n",
    "    num_batches = num_samples // batch_size\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        mini_x = training_x[start_idx:end_idx]\n",
    "        mini_y = training_y[start_idx:end_idx]\n",
    "        yield mini_x, mini_y\n",
    "    \n",
    "    # Handle the last batch with fewer samples\n",
    "    if num_samples % batch_size != 0:\n",
    "        start_idx = num_batches * batch_size\n",
    "        mini_x = training_x[start_idx:]\n",
    "        mini_y = training_y[start_idx:]\n",
    "        yield mini_x, mini_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVMwqheRpdXo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Single_layer_classifier():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        # Give the instance a weight matrix, initialized randomly\n",
    "        # One possible strategy for a good initialization is Normal (0, σ) where σ = 1e-3.\n",
    "        mean=0\n",
    "        std_dev=10**(-3)\n",
    "        self.W=np.random.normal(loc=mean,scale=std_dev,size=(input_size,output_size))\n",
    "        # print(self.W)\n",
    "        # Try experimenting with different values of σ.\n",
    "        \n",
    "        # Xavier init\n",
    "        # std_dev_2=2/(input_size+output_size)\n",
    "        # W=np.random.normal(loc=mean,scale=std_dev_2,size=(input_size,output_size))\n",
    "        # print(W)\n",
    "        \n",
    "        \n",
    "    # Define the forward function\n",
    "    def forward(self, input_x):\n",
    "        \n",
    "        # get the scores\n",
    "        ## WRITE CODE HERE\n",
    "        self.scores=np.dot(input_x,self.W)\n",
    "        return self.scores\n",
    "        # return scores\n",
    "    \n",
    "    # Similarly a backward function\n",
    "    # we define 2 backward functions (as Loss = L_data + L_reg, grad(Loss) = grad(L1) + grad(L2))\n",
    "    \n",
    "    def backward_Ldata(self, grad_from_loss):\n",
    "        \n",
    "        # this function returns a matrix of the same size as the weights, \n",
    "        # where each element is the partial derivative of the loss w.r.t. the corresponding element of W\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        # n=np.size(self.scores)\n",
    "        # tmp=np.tile(self.scores,(n,1))\n",
    "        # grad_matrix=np.dot(tmp * (np.identity(n) - np.transpose(tmp)),grad_from_loss)\n",
    "        # print(grad_matrix.shape)\n",
    "        n = np.size(self.scores)\n",
    "        tmp = np.tile(self.scores, (n, 1))\n",
    "        return np.dot(tmp * (np.identity(n) - np.transpose(tmp)), grad_from_loss)\n",
    "        # return grad_matrix\n",
    "        \n",
    "    def backward_Lreg(self):\n",
    "        \n",
    "        # this function returns a matrix of the same size as the weights, \n",
    "        # where each element is the partial derivative of the regularization-term\n",
    "        # w.r.t. the corresponding element of W\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return grad_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BM3EhmY0pdYZ"
   },
   "outputs": [],
   "source": [
    "# Implement the Softmax loss function\n",
    "def loss_function(input_y,scores):\n",
    "\n",
    "    ## WRITE CODE HERE  \n",
    "    #softmax\n",
    "    exp_scores=np.exp(scores)\n",
    "    # print(scores)\n",
    "    softmax_prob=exp_scores/np.sum(exp_scores,axis=1,keepdims=True)\n",
    "    # softmax_loss_func=-np.log(softmax_prob)\n",
    "    # print(softmax_prob.shape)\n",
    "    loss=softmax_prob\n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_backward(loss,scores):\n",
    "    # This part deals with the gradient of the loss w.r.t the output of network\n",
    "    # for example, in case of softmax loss(-log(q_c)), this part gives grad(loss) w.r.t. q_c\n",
    "    # pass this to backward_ldata\n",
    "    \n",
    "    ## WRITE CODE HERE    \n",
    "    \n",
    "    softmax=loss\n",
    "    # print(softmax)\n",
    "    n,m=softmax.shape\n",
    "    output_gradient=np.zeros((n,m,m))\n",
    "    # print(n)\n",
    "    # print(m)\n",
    "    # print(output_gradient.shape)\n",
    "    for i in range (n):\n",
    "        for j in range(m):\n",
    "            for k in range(m):\n",
    "                output_gradient[i,j,k]=softmax[i,j]*(int(j==k)-softmax[i,k])\n",
    "    return output_gradient\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create utility functions for calculating training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minibatch=get_minibatch()\n",
    "classifier=Single_layer_classifier(784,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhpaaUvJpdZJ"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1000,1000) (10,100000) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m output_gradient\u001b[38;5;241m=\u001b[39mloss_backward(loss,scores)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# print(output_gradient)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m classifier\u001b[38;5;241m.\u001b[39mbackward_Ldata(output_gradient)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[88], line 46\u001b[0m, in \u001b[0;36mSingle_layer_classifier.backward_Ldata\u001b[1;34m(self, grad_from_loss)\u001b[0m\n\u001b[0;32m     44\u001b[0m n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores)\n\u001b[0;32m     45\u001b[0m tmp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores, (n, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(tmp \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39midentity(n) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(tmp)), grad_from_loss)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1000,1000) (10,100000) "
     ]
    }
   ],
   "source": [
    "# Finally the trainer:\n",
    "# Make an instance of Single_layer_classifier\n",
    "# Train for t epochs:\n",
    "###  Train on the train-set obtained from train-validation split\n",
    "###  Use the mini-batch generator to get each mini-batch\n",
    "\n",
    "for iter_num,(input_x , input_y) in enumerate(minibatch):\n",
    "    # Write code for each iteration of the training\n",
    "    # print(iter_num)\n",
    "    input_x_reshaped=np.reshape(input_x,(100,28*28))\n",
    "    # Forward pass\n",
    "    scores=classifier.forward(input_x_reshaped)\n",
    "    # print(scores)\n",
    "    loss=loss_function(input_y,scores)\n",
    "    # print(loss)\n",
    "\n",
    "    # Backward pass\n",
    "    output_gradient=loss_backward(loss,scores)\n",
    "    # print(output_gradient)\n",
    "    classifier.backward_Ldata(output_gradient)\n",
    "    break\n",
    "    # Update weights\n",
    "    \n",
    "    # Log the training loss value and training accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training loss and training accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RH22BqefpdZ4"
   },
   "source": [
    "### Find the accuracy on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wMDvvA4pdbN"
   },
   "outputs": [],
   "source": [
    "# The next step is to find the optimal value for lambda, number of epochs, learning rate and batch size. \n",
    "# CHOSE ANY TWO from the above mentioned to tune.\n",
    "# Create plot and table to show the effect of the hparams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDrq0GfKpdb8"
   },
   "source": [
    "### Report final performance on MNIST test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best performing class and the worst performing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Emb3Bo_NpdUl"
   },
   "source": [
    "# Training a Linear Classifier on MNIST from scikit-learn\n",
    "\n",
    "In this section you have to train a linear classifier from the scikit-learn library and compare its results against your implementation.\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # WRITE CODE HERE \n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_train=training_x\n",
    "Y_train=training_y\n",
    "x_test=test_data\n",
    "y_test=test_label\n",
    "regression_model=LinearRegression()\n",
    "X_train_reshaped=np.reshape(X_train,(42000,28*28))\n",
    "x_test_reshapd=np.reshape(x_test,(10000,28*28))\n",
    "# print(X_train_reshaped.shape)\n",
    "# print(Y_train.shape)\n",
    "# print(x_test.shape)\n",
    "regression_model.fit(X_train_reshaped,Y_train)\n",
    "y_pred=regression_model.predict(x_test_reshapd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the training and test accuracies for the your implementation and linear classifier from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any additional observations / comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS Question\n",
    "### Observe the effect on test set accuracy by changing the number of training samples.\n",
    "### Train on 10%, 20% and 50% training data and plot the percentage of training data v.s. the test accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXFJTTnkpdcp"
   },
   "source": [
    "# Implementing Backpropagation\n",
    "\n",
    "Now that you have had some experience with single layer networks, we can proceed to more complex architectures. But first we need to completely understand and implement backpropagation.\n",
    "\n",
    "## Backpropagation:\n",
    "\n",
    "Simply put, a way of computing gradients of expressions through repeated application of chain rule. If\n",
    "$$\n",
    "L = f (g (h (\\textbf{x})))\n",
    "$$\n",
    "then, by the chain rule we have:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\textbf{x}} = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial \\textbf{x}} \n",
    "$$\n",
    "\n",
    "** Look into the class Lecture for more detail **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UixM41Rppdcz"
   },
   "source": [
    "# Question 2 : Scalar Backpropagation\n",
    "\n",
    "Evaluate the gradient of the following function w.r.t. the input:\n",
    "\n",
    "$$ f(x,y,z) =  log(\\sigma(\\frac{cos(\\pi \\times x)+sin(\\pi \\times y/2)}{tanh(z^2)}))$$\n",
    "where $\\sigma$ is the sigmoid function. Find gradient for the following inputs:\n",
    "  * $(x,y,z)$ =  (2,4,1)\n",
    "  * $(x,y,z)$ =  (9,14,3)\n",
    "  * $(x,y,z)$ =  (128,42,666)\n",
    "  * $(x,y,z)$ =  (52,14,28)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uu9zd5PLpdc_"
   },
   "outputs": [],
   "source": [
    "# To solve this problem, construct the computational graph\n",
    "# Write a class with forward and backward functions, for each node if you like\n",
    "# For eg:\n",
    "class Cos():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,x):\n",
    "        self.x=x\n",
    "        self.output=np.cos(np.pi*x)\n",
    "        return self.output\n",
    "    def backward(self,gradient_value):\n",
    "        self.gradient=gradient_value*(-1 * np.sin(x))\n",
    "        return self.gradient\n",
    "    \n",
    "class Sin():\n",
    "    # def __init__(\n",
    "    def forward(self,y):\n",
    "        self.y=y\n",
    "        self.output=np.sin(np.pi * (y/2))\n",
    "        return self.output\n",
    "    def backward(self,gradient_value):\n",
    "        self.gradient=gradient_value*np.cos(y)\n",
    "    \n",
    "class Tanh():\n",
    "    def forward(self,z):\n",
    "        self.z=z\n",
    "        self.output=np.tanh(z**2)\n",
    "        return self.output\n",
    "    def backward(self,gradient_value):\n",
    "        self.gradient=gradient_value*(1-np.tanh(z)**2)\n",
    "        return self.gradient\n",
    "        \n",
    "\n",
    "class Log():\n",
    "    def forward(self,sigmoid_parameter):\n",
    "        self.sigmoid_parameter=sigmoid_parameter\n",
    "        self.output=np.log(sigmoid_parameter)\n",
    "        return self.output\n",
    "    def backward(self,gradient_value):\n",
    "        self.gradient=gradient_value/sigmoid_parameter\n",
    "        return self.gradient\n",
    "\n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,cos_param,sin_param,tanh_param):\n",
    "        self.cos_param=cos_param\n",
    "        self.sin_param=sin_param\n",
    "        self.tanh_param=tanh_param\n",
    "        self.output=(cos_param+sin_param)/tanh_param\n",
    "        # save values useful for backpropagation\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,gradient_value):\n",
    "        self.gradient=gradient_value*(self.output*(1-self.output))\n",
    "        return self.gradient\n",
    "\n",
    "        \n",
    "# CAUTION: Carefully treat the input and output dimension variation. At worst, handle them with if statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyqeWPF0pddy"
   },
   "outputs": [],
   "source": [
    "# Now write the class func\n",
    "# which constructs the graph (all operators), forward and backward functions.\n",
    "\n",
    "class Func():\n",
    "    def __init__(self):\n",
    "        # construct the graph here\n",
    "        # assign the instances of function modules to self.var\n",
    "        self.cos=Cos()\n",
    "        self.sin=Sin()\n",
    "        self.tanh=Tanh()\n",
    "        self.log=Log()\n",
    "        self.sigmoid=Sigmoid()\n",
    "    def forward(self,x,y,z):\n",
    "        # Using the graph element's forward functions, get the output. \n",
    "        cos_param=self.cos.forward(x)\n",
    "        sin_param=self.sin.forward(y)\n",
    "        tanh_param=self.tanh.forward(z)\n",
    "        sigmoid_param=self.sigmoid.forward(cos_param,sin_param,tanh_param)\n",
    "        log_param=self.log.forward(sigmoid_param)\n",
    "        output=log_param\n",
    "        return output\n",
    "    \n",
    "    def backward(output):\n",
    "        # Use the saved outputs of each module, and backward() function calls\n",
    "        gradient_of_log=1\n",
    "        return [grad_x,grad_y,grad_z]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2723414689118313"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func=Func()\n",
    "func.forward(2,4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYE4dEd-pdef"
   },
   "source": [
    "## Question 3 : Modular Vector Backpropagation\n",
    "\n",
    "* Construct a Linear Layer module, implementing the forward and backward functions for arbitrary sizes.\n",
    "* Construct a ReLU module, implementing the forward and backward functions for arbitrary sizes.\n",
    "* Create a 2 layer MLP using the constructed modules.\n",
    "\n",
    "* Modifying the functions built in Question 1 , train this two layer MLP for the same data set, MNIST, with the same train-val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0viOMFfFpden"
   },
   "outputs": [],
   "source": [
    "# Class for Linear Layer (If you're stuck, you can refer to code of PyTorch/Tensorflow packages) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmlWuzv1pdfK"
   },
   "outputs": [],
   "source": [
    "# Class for ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppSfRLySpdfv"
   },
   "outputs": [],
   "source": [
    "# Your 2 layer MLP \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MyDbNSeRpdgL"
   },
   "outputs": [],
   "source": [
    "# Train the MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training loss and training accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same utility functions defined in the previous question\n",
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RH22BqefpdZ4"
   },
   "source": [
    "### Find the accuracy on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wMDvvA4pdbN"
   },
   "outputs": [],
   "source": [
    "# Find the optimal value of learning rate and batch size. \n",
    "# Use the same tuning strategy as the previous question\n",
    "# Create plot and table to show the effect of the hparams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDrq0GfKpdb8"
   },
   "source": [
    "### Report final performance on MNIST test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best performing class and the worst performing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any additional observations / comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS Question\n",
    "### Observe the effect on test set accuracy by changing the number of training samples.\n",
    "### Train on 10%, 20% and 50% training data and plot the percentage of training data v.s. the test accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KX-cXeY3pdhr"
   },
   "source": [
    "\n",
    "# Implementing a Convolution Module with Numpy\n",
    "\n",
    "* This topic will require you to implement the Convolution operation using Numpy.\n",
    "* We will use the Module for tasks like Blurring.\n",
    "* Finally, we implement Backpropagation for the convolution module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHeieqicpdhz"
   },
   "source": [
    "## Question 4\n",
    "\n",
    "* Implement a naive Convolution module, with basic functionalities: kernel_size, padding, stride and dilation\n",
    "  \n",
    "* Test out the convolution layer by using it to do gaussian blurring on 10 random images of CIFAR-10 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YoVvSEj0pdh5"
   },
   "outputs": [],
   "source": [
    "## Define a class Convolution Layer, which is initialized with the various required params:\n",
    "import numpy as np\n",
    "class Convolution_Layer():\n",
    "    \n",
    "    def __init__(self,input_image , filter_size, bias=True, stride=1, padding=0, dilation=1):\n",
    "        \n",
    "\n",
    "    def forward(self,input_image):\n",
    "        # Input Proprocess(According to pad etc.) Input will be of size (Batch_size, in_channels, inp_height, inp_width)\n",
    "        \n",
    "        # Reminder: Save Input for backward-prop\n",
    "        # Simple Conv operation:\n",
    "        # Loop over every location in inp_height * inp_width for the whole batch\n",
    "        \n",
    "        # Output will be of the size (Batch_size, out_channels, out_height, out_width)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_of_output_size):\n",
    "        \n",
    "        # Naive Implementation\n",
    "        # Speed is not a concern\n",
    "        # Hint: gradients from each independant operation can be summed\n",
    "        \n",
    "        #  return gradient of the size of the weight kernel\n",
    "        return grad\n",
    "    \n",
    "    def set_weights(self, new_weights):\n",
    "        ## Replace the set of weights with the given 'new_weights'\n",
    "        ## use this for setting weights for blurring, bilateral filtering etc. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDoqiua8pdia"
   },
   "source": [
    "### Download CIFAR-10 images and load it in a numpy array (https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n"
     ]
    }
   ],
   "source": [
    "# WRITE CODE HERE\n",
    "import pickle\n",
    "def load_CIFAR():\n",
    "    file_path='cifar-10-batches-py\\data_batch_1'\n",
    "    with open(file_path,'rb') as file:\n",
    "        data=pickle.load(file,encoding='bytes')\n",
    "    data=data[b'data']\n",
    "    print(len(data[0]))\n",
    "load_CIFAR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a conv layer. Set weights for gaussian blurring (do not train the filter for this part). Visualise the filters using matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate output for the first 5 images of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use matplotlib to show the input and corresponding blurred output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmsPeOEOpdi8"
   },
   "source": [
    "## Question 5\n",
    "<br>\n",
    "Now we will use this module for training a simple Convolution Layer using CIFAR-10 images. \n",
    "\n",
    "* The goal is to learn a set of weights, by using the backpropagation function created. To test the backpropagation, instead of training a whole network, we will train only a single layer.\n",
    "  * Instantiate a Convolution  layer $C_0$ with 20 filters, each with size 5$\\times$5 (RGB image, so 3 input channels). Load the given numpy array of size (20,3,5,5), which represents the weights of a convolution layer. Set the given values as the filter weights for $C_0$. Take 100 CIFAR-10 images. Save the output of these 100 images generated from this Convolution layer $C_0$. \n",
    "  \n",
    "  * Now, initialize a new convolution layer $C$ with weight values sampled from uniform distribution [-1,1]. Use the $L_2$ loss between the output of this layer $C$ and the output generated in the previous step to learn the filter weights of $C_0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8dGnMHOpdjD"
   },
   "outputs": [],
   "source": [
    "## Load filter weights from given numpy array \"C0_weights.npy\".\n",
    "## Init a conv layer C_0 with these given weights\n",
    "\n",
    "## For all images get output. Store in numpy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhhHA4UCpdji"
   },
   "outputs": [],
   "source": [
    "# for part 2 we need to write a class for the  L2 loss\n",
    "class L2_loss():\n",
    "    def ___init__(self):\n",
    "    \n",
    "    def forward(self, C0_output,C_output):\n",
    "        # Conv. output is of dimension (batchsize,channels,height,width)\n",
    "        # calculate the L2 norm of (C0_output - C_output)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self,output_grad):\n",
    "        # from the loss, and the conv. output, get the grad at each location\n",
    "        # The grad is of the shape (batchsize,channels,height,width)\n",
    "        return grad\n",
    "\n",
    "# Now Init a new conv layer C and a L2 loss layer\n",
    "\n",
    "# Train the new conv-layer C using the L2 loss to learn C_0, i.e., the set of given weights.\n",
    "# Use mini-batches if required\n",
    "\n",
    "\n",
    "# Print L2 dist between output from the new trained convolution layer C and the outputs generated from C_0.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "DLCV_Assignment_2-edit.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "6945496b114ae6c8dbcad4f53e4fc084fdd8255c08c862c02a1152642bece680"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
