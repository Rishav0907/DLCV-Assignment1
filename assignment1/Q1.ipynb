{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "425f2b9f-ded3-4a08-923f-7a94adbfae99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load The Mnist data:\n",
    "# Download data from http://yann.lecun.com/exdb/mnist/\n",
    "# load the data.\n",
    "import idx2numpy\n",
    "data=idx2numpy.convert_from_file('train-images.idx3-ubyte')\n",
    "label=idx2numpy.convert_from_file('train-labels.idx1-ubyte')\n",
    "test_data=idx2numpy.convert_from_file('t10k-images.idx3-ubyte')\n",
    "test_label=idx2numpy.convert_from_file('t10k-labels.idx1-ubyte')\n",
    "# maintain a train-val split\n",
    "train_size=0.7*len(data)\n",
    "training_x=data[0:int(train_size)]\n",
    "# print(len(training_x))\n",
    "crossval_x=data[int(train_size):len(data)]\n",
    "training_y=label[0:int(train_size)]\n",
    "crossval_y=label[int(train_size):len(data)]\n",
    "# Now, write a generator that yields (random) mini-batches of the input data\n",
    "# Do not use same set of mini-batches for different epochs\n",
    "    \n",
    "def get_minibatch(training_x=training_x, training_y=training_y):\n",
    "    batch_size=100\n",
    "    num_samples = len(training_x)\n",
    "    num_batches = num_samples // batch_size\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        mini_x = training_x[start_idx:end_idx]\n",
    "        mini_y = training_y[start_idx:end_idx]\n",
    "        yield mini_x, mini_y\n",
    "    \n",
    "    # Handle the last batch with fewer samples\n",
    "    if num_samples % batch_size != 0:\n",
    "        start_idx = num_batches * batch_size\n",
    "        mini_x = training_x[start_idx:]\n",
    "        mini_y = training_y[start_idx:]\n",
    "        yield mini_x, mini_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "71d16811-494c-4841-9708-fbc866ffa8b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Single_layer_classifier():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        # Give the instance a weight matrix, initialized randomly\n",
    "        # One possible strategy for a good initialization is Normal (0, σ) where σ = 1e-3.\n",
    "        mean=0\n",
    "        std_dev=10**(-3)\n",
    "        self.W=np.random.normal(loc=mean,scale=std_dev,size=(input_size,output_size))\n",
    "        # print(self.W)\n",
    "        # print(self.W)\n",
    "        # Try experimenting with different values of σ.\n",
    "        \n",
    "        # Xavier init\n",
    "        # std_dev_2=2/(input_size+output_size)\n",
    "        # W=np.random.normal(loc=mean,scale=std_dev_2,size=(input_size,output_size))\n",
    "        # print(W)\n",
    "        \n",
    "        \n",
    "    # Define the forward function\n",
    "    def forward(self, input_x):\n",
    "        \n",
    "        # get the scores\n",
    "        ## WRITE CODE HERE\n",
    "        self.scores=np.dot(input_x,self.W)\n",
    "        # print(self.scores)\n",
    "        return self.scores\n",
    "        # return scores\n",
    "    \n",
    "    # Similarly a backward function\n",
    "    # we define 2 backward functions (as Loss = L_data + L_reg, grad(Loss) = grad(L1) + grad(L2))\n",
    "    \n",
    "    def backward_Ldata(self, grad_from_loss):\n",
    "        \n",
    "        # this function returns a matrix of the same size as the weights, \n",
    "        # where each element is the partial derivative of the loss w.r.t. the corresponding element of W\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return grad_matrix\n",
    "        \n",
    "    def backward_Lreg(self):\n",
    "        \n",
    "        # this function returns a matrix of the same size as the weights, \n",
    "        # where each element is the partial derivative of the regularization-term\n",
    "        # w.r.t. the corresponding element of W\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return grad_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bbc01601-7296-472b-8fe8-2ecd70a6266a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implement the Softmax loss function\n",
    "def loss_function(input_y,scores):\n",
    "\n",
    "    ## WRITE CODE HERE  \n",
    "    #softmax\n",
    "    exp_scores=np.exp(scores)\n",
    "    # print(scores)\n",
    "    softmax_prob=exp_scores/np.sum(exp_scores,axis=0,keepdims=True)\n",
    "    # softmax_loss_func=-np.log(softmax_prob)\n",
    "    # print(softmax_prob)\n",
    "    loss=softmax_prob\n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_backward(loss):\n",
    "    # This part deals with the gradient of the loss w.r.t the output of network\n",
    "    # for example, in case of softmax loss(-log(q_c)), this part gives grad(loss) w.r.t. q_c\n",
    "    # pass this to backward_ldata\n",
    "    \n",
    "    ## WRITE CODE HERE    \n",
    "    # print(loss.shape)\n",
    "    # grad_from_loss=loss *  (np.eye(10)-loss)\n",
    "    print(loss.shape)\n",
    "    N=loss.shape[0]\n",
    "    \n",
    "    # return grad_from_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "892a10c2-a70e-4bee-9712-0fffd997cf17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minibatch=get_minibatch()\n",
    "classifier=Single_layer_classifier(784,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6adc2ed8-1ae4-468c-9175-561787558920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     scores\u001b[38;5;241m=\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mforward(input_x_reshaped)\n\u001b[0;32m     16\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss_function(input_y,scores)\n\u001b[1;32m---> 17\u001b[0m     loss_backward(loss)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[71], line 25\u001b[0m, in \u001b[0;36mloss_backward\u001b[1;34m(loss)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     24\u001b[0m N\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 25\u001b[0m D\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(D)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Finally the trainer:\n",
    "# Make an instance of Single_layer_classifier\n",
    "# Train for t epochs:\n",
    "###  Train on the train-set obtained from train-validation split\n",
    "###  Use the mini-batch generator to get each mini-batch\n",
    "\n",
    "for iter_num,(input_x , input_y) in enumerate(minibatch):\n",
    "    # Write code for each iteration of the training\n",
    "    # print(iter_num)\n",
    "    # print(len(input_x))\n",
    "    for data in range(len(input_x)):\n",
    "        # print((input_x[data].shape))\n",
    "        input_x_reshaped=np.reshape(input_x[data],(784))\n",
    "        # print(np.transpose(input_x_reshaped))\n",
    "        scores=classifier.forward(input_x_reshaped)\n",
    "        loss=loss_function(input_y,scores)\n",
    "        loss_backward(loss)\n",
    "        break\n",
    "    break\n",
    "    # Forward pass\n",
    "   \n",
    "    # break\n",
    "    # Backward pass\n",
    "\n",
    "    # Update weights\n",
    "    \n",
    "    # Log the training loss value and training accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d1f62-22dc-4346-8325-bd50e182d394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c050a-a08f-4b4d-8955-b80fc88ceff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
