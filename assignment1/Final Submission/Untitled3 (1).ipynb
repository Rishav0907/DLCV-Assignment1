{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e19f6f-c29a-4825-a570-9a9285041dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Linear Layer (If you're stuck, you can refer to code of PyTorch/Tensorflow packages) \n",
    "import numpy as np\n",
    "\n",
    "class LinearLayer:\n",
    "    def __init__(self,input_layer_size,output_layer_size):\n",
    "        mean=0\n",
    "        std_dev=10**(-3)\n",
    "        self.weight_matrix=np.random.normal(loc=mean,scale=std_dev,size=(input_layer_size,output_layer_size)) \n",
    "        # 784 x 128 for layer 1\n",
    "        # 128 x 10 for layer 2\n",
    "        # print(self.weight_matrix)\n",
    "    def forward(self,input_data):\n",
    "        self.input_data=input_data\n",
    "        # 100 x 784 for layer 1\n",
    "        # 100 x 128 for layer 2\n",
    "        self.score=np.dot(input_data,self.weight_matrix) \n",
    "        # 100 x 128 score shape for layer 1\n",
    "        # 100 x 10 score shape for layer 2\n",
    "        return self.score\n",
    "    def backward(self,gradient_output):\n",
    "        self.gradient_wrt_weights=np.dot(self.input_data.T,gradient_output)\n",
    "        # 128 x 10 for layer 2\n",
    "        return np.dot(gradient_output,self.weight_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ef958-aa83-4c48-ad09-eb40ef2450fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for ReLU\n",
    "class ReLU:\n",
    "    def forward(self,input_data):\n",
    "        self.input_data=input_data\n",
    "        self.out_data=np.maximum(0,input_data)\n",
    "        # print(self.out_data.shape) # 100 x 128\n",
    "        return self.out_data \n",
    "    def backward(self,out_grad):\n",
    "        # print(out_grad.shape)\n",
    "        # out_grad : 128 x 10\n",
    "        return out_grad*(self.input_data>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813f901-2909-4e46-9274-64e6950112ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for MLP\n",
    "class MLP:\n",
    "    def __init__(self,input_layer_size,hidden_layer_size,output_layer_size):\n",
    "        self.linear_layer1=LinearLayer(input_layer_size,hidden_layer_size) # 784 x 128\n",
    "        self.relu=ReLU()\n",
    "        self.linear_layer_2=LinearLayer(hidden_layer_size,output_layer_size) # 128 x 10\n",
    "    def forward(self,input_X):\n",
    "        layer1_output=self.linear_layer1.forward(input_X)\n",
    "        # print(layer1_output)\n",
    "        relu_layer_output=self.relu.forward(layer1_output)\n",
    "        layer2_output=self.linear_layer_2.forward(relu_layer_output)\n",
    "        return layer2_output\n",
    "    def backward(self,gradient_output):\n",
    "        gradient_output1=self.linear_layer_2.backward(gradient_output)\n",
    "        gradient_output2=self.relu.backward(gradient_output1)\n",
    "        gradient_output3=self.linear_layer1.backward(gradient_output2)\n",
    "        return gradient_output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769eede-7976-4e02-bede-69f4151bedff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing one hot encoding\n",
    "def one_hot_encoding(y_labels,num_classes):\n",
    "    batch_size= len(y_labels)\n",
    "    one_hot = np.zeros((batch_size, num_classes))\n",
    "    one_hot[np.arange(batch_size), y_labels] = 1\n",
    "    return one_hot\n",
    "    \n",
    "def softmax1(scores):\n",
    "    #softmax\n",
    "    exp_scores=np.exp(scores)\n",
    "    # print(scores)\n",
    "    softmax_prob=exp_scores/np.sum(exp_scores,axis=1,keepdims=True)\n",
    "    return softmax_prob\n",
    "    \n",
    "def cross_entropy_loss(input_y,scores):\n",
    "\n",
    "    # num_of_datapoints = input_y.shape[0] # Number of samples\n",
    "    # softmax_prob=softmax1(scores)\n",
    "    # log_likelihood = -np.log(softmax_prob[range(num_of_datapoints), input_y.argmax(axis=1)])\n",
    "    # loss = np.sum(log_likelihood) / num_of_datapoints\n",
    "    # return loss\n",
    "    total_samples = input_y.shape[0] # Number of samples\n",
    "    softmax_prob=softmax(scores)\n",
    "    loss= -np.mean(np.sum(input_y * np.log(softmax_prob), axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292282e7-b239-4ccc-b96d-36edf487ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses=[]\n",
    "training_accuracy=[]\n",
    "def linear_trainer(x_train,y_train,LEARNING_RATE=0.01,BATCH_SIZE=100):\n",
    "    # LEARNING_RATE=0.001\n",
    "    epochs=10\n",
    "    mlp=MLP(input_layer_size=784,hidden_layer_size=128,output_layer_size=10)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        miniBatch=get_minibatch(batch_size_param=BATCH_SIZE,training_x=x_train,training_y=y_train)\n",
    "        for index,(input_x,input_y) in enumerate(miniBatch):\n",
    "            input_x_reshaped=np.reshape(input_x,(BATCH_SIZE,28*28))\n",
    "            # print(input_x_reshaped.shape) # 100 x 784\n",
    "            # break\n",
    "            predicted_data=mlp.forward(input_x_reshaped) # 100 x 10\n",
    "            #loss\n",
    "            input_y_one_hot_encoded=one_hot_encoding(input_y,num_classes=10) # 100 x 10\n",
    "            loss=cross_entropy_loss(input_y_one_hot_encoded,predicted_data)\n",
    "            epoch_loss+=loss\n",
    "            mlp.backward(predicted_data - input_y_one_hot_encoded) # passing a 100 x 10 tensor\n",
    "        \n",
    "            mlp.linear_layer1.weight_matrix -= LEARNING_RATE*mlp.linear_layer1.gradient_wrt_weights\n",
    "            mlp.linear_layer_2.weight_matrix -= LEARNING_RATE*mlp.linear_layer_2.gradient_wrt_weights\n",
    "            \n",
    "            if index % BATCH_SIZE == 0:\n",
    "                epoch_loss /= BATCH_SIZE\n",
    "                train_accuracy = accuracy(input_x, input_y_one_hot_encoded, mlp,BATCH_SIZE)\n",
    "                training_losses.append(epoch_loss)\n",
    "                # epoch_loss=0\n",
    "                training_accuracy.append(train_accuracy*100)\n",
    "        return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6cb97-7d95-4915-aa7c-477941ff725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier=linear_trainer(x_train=training_x,y_train=training_y)\n",
    "print(f\"{np.mean(train_accuracy)*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
