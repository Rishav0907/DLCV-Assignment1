{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ad34b6-1096-4e05-a543-87fbd785f5ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "data=idx2numpy.convert_from_file('train-images.idx3-ubyte')\n",
    "label=idx2numpy.convert_from_file('train-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0a9740-feb7-405f-956e-2da2ee09fe12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4441a8-8c9d-4137-a3f7-955ca7528e90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319efcc9-a5ef-4c70-b6f4-eafba9c1775d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load The Mnist data:\n",
    "# Download data from http://yann.lecun.com/exdb/mnist/\n",
    "# load the data.\n",
    "import idx2numpy\n",
    "data=idx2numpy.convert_from_file('train-images.idx3-ubyte')\n",
    "label=idx2numpy.convert_from_file('train-labels.idx1-ubyte')\n",
    "# maintain a train-val split\n",
    "train_size=0.7*len(data)\n",
    "training_x=data[0:int(train_size)]\n",
    "crossval_x=data[int(train_size):len(data)]\n",
    "training_y=label[0:int(train_size)]\n",
    "crossval_y=label[int(train_size):len(data)]\n",
    "# Now, write a generator that yields (random) mini-batches of the input data\n",
    "# Do not use same set of mini-batches for different epochs\n",
    "    \n",
    "def get_minibatch(training_x=training_x, training_y=training_y):\n",
    "    ## Read about Python generators if required.\n",
    "\n",
    "    ## WRITE CODE HERE\n",
    "    BATCH_SIZE=100\n",
    "    for i in range(0,int(train_size),BATCH_SIZE):\n",
    "        mini_x=training_x[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        mini_y=training_y[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        yield mini_x,mini_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "123a4ef6-68ea-452d-8c44-b89159fbafee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "minibatch=get_minibatch()\n",
    "for k,(i,j) in enumerate(minibatch):\n",
    "    print(k)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bad1f7-95db-47c2-b6de-7794b4ffc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Single_layer_classifier():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        # Give the instance a weight matrix, initialized randomly\n",
    "        # One possible strategy for a good initialization is Normal (0, σ) where σ = 1e-3.\n",
    "        mean=0\n",
    "        std_dev=10**(-3)\n",
    "        W=np.random.normal(loc=mean,scale=std_dev,size=(input_size,output_size)\n",
    "        print(W)\n",
    "        # Try experimenting with different values of σ.\n",
    "        \n",
    "    # Define the forward function\n",
    "    def forward(self, input_x):\n",
    "        \n",
    "        # get the scores\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    # Similarly a backward function\n",
    "    # we define 2 backward functions (as Loss = L_data + L_reg, grad(Loss) = grad(L1) + grad(L2))\n",
    "    \n",
    "    def backward_Ldata(self, grad_from_loss):\n",
    "        \n",
    "        # this function returns a matrix of the same size as the weights, \n",
    "        # where each element is the partial derivative of the loss w.r.t. the corresponding element of W\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return grad_matrix\n",
    "        \n",
    "    def backward_Lreg(self):\n",
    "        \n",
    "        # this function returns a matrix of the same size as the weights, \n",
    "        # where each element is the partial derivative of the regularization-term\n",
    "        # w.r.t. the corresponding element of W\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return grad_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
